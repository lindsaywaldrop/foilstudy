---
title: "Creating Surrogates"
output:
  html_notebook: default
---

```{r setup, echo=FALSE}
library(pracma)
library(spacefillr)
library(ggplot2)
library(patchwork)
library(tidyr)
```

This document walks through the process of creating performance spaces from each of the three methods (grid sampling, generalized polynomial chaos, and neural network) using the same number of simulations for each method (343 total) across three parameters (Reynolds number $Re$, angle of attack $aoa$, and airfoil camber $camber$). The performance metric chosen is the coefficient of lift ($C_L$). The end result is a figure presented in the review paper comparing the resolutions of each method.

Note: If you wish to knit this document and are receiving an error about not finding a file, please select "Project directory" under Knit > Knit Directory. 

## Figure from publication

The figure from the review paper consists of each of the below methods plotted together. For convenience, we've included the results from completed surrogates here. To rerun everything, please proceed to each section below.

```{r}
# Load custom functions
source("./src/r-scripts/gPC_functions.R")
source("./src/r-scripts/nn_functions.R")

# Define parameter ranges
Re_range <- c(1e6, 1e9)
aoa_range <- c(0, 15)
camber_range <- c(0.005, 0.20)

#### Grid Sampling Subplot ####
# Read in the results of the simulations:
grid_results <- read.csv("./results/grid_noLogRe_results.csv")
# Reorder the results so that they are in the same order as the parameter file:
grid_results <- grid_results[order(grid_results$sim), ]
# Read in the parameter set:
grid_params <- read.csv("./data/parameters/grid_Params_noLogRe.csv", header = F)
colnames(grid_params) <- c("Re", "aoa", "camber")
# Bind the parameters and results together:
grid_results <- cbind("sim" = grid_results$sim, grid_params, grid_results[, 2:4])
# Clean up 
rm(grid_params)

# Construct grid-sampling subplot
grid_plot <- ggplot(grid_results[grid_results$Re == 500500000,], aes(aoa, camber, fill = CL)) +
  geom_tile() +
  scale_fill_viridis_c(name = expression(C[L])) + 
  xlab("Angle of attack") + ylab("Camber") +
  ggtitle("Grid Sampling") + 
  theme_bw()

#### gPC Expansion subplot ####

# Load previously generated gPC coefficients and alpha mat
s_coeffs_CL <- as.matrix(read.csv("./src/r-scripts/gpc-data-2025-09-11/gPC_coefficients.csv"))
alpha_mat <- as.matrix(read.csv("./src/r-scripts/gpc-data-2025-09-11/gPC_alphamat.csv"))

# Generate surrogate performance space
y1_vec <- seq(aoa_range[1], aoa_range[2], length.out = 100)
z1_vec <- seq(camber_range[1], camber_range[2], length.out = 100)
mesh_out <- meshgrid(y1_vec, z1_vec)
y1_mesh <- mesh_out$X
z1_mesh <- mesh_out$Y
x1_const <- 500500000
x1_mesh <- matrix(x1_const, nrow = 100, ncol = 100)
surrogate_dat <- data.frame("Re" = as.vector(x1_mesh), "aoa" = as.vector(y1_mesh), 
                            "camber" = as.vector(z1_mesh))

surrogate_dat$CL_surrogate <- gPC_expansion(surrogate_dat, s_coeffs_CL, alpha_mat, 
                             Re_range, aoa_range, camber_range)

predict_gpc_plot <- ggplot(surrogate_dat, aes(aoa, camber, fill = CL_surrogate)) + 
  geom_tile() + 
  scale_fill_viridis_c(name = expression(C[L])) + 
  xlab("Angle of attack") + ylab("Camber") +
  ggtitle("gPC Expansion") + 
  theme_bw() #+ 
  #theme(legend.position = "bottom")


#### Neural Network subplot ####
trained_model <- load_trained_model("./src/r-scripts/nn-data-2025-08-26/")
lambda <- 1.7
x1_const <- 500500000
x1_const_scaled <- 0.5
x1_mesh_scaled <- matrix(x1_const_scaled, nrow = 100, ncol = 100)
y1_vec <- seq(0, 15, length.out = 100)
z1_vec <- seq(0, 0.2, length.out = 100)
mesh_out <- meshgrid(line_fxn(y1_vec), line_fxn(z1_vec))
mesh_out_real <- meshgrid(y1_vec, z1_vec)
y1_mesh <- mesh_out$X
z1_mesh <- mesh_out$Y


viz_it <- create_2D(x1_mesh_scaled, y1_mesh, z1_mesh, trained_model, true_dat = F)

ztest <- unscale_output_data(as.vector(viz_it$f_prediction), 
                             as.vector(trained_model$min_z), 
                             as.vector(trained_model$max_z))
ztest <- untransform_output_data(ztest, option = "box-cox-y", lambda, 
                                  dat_mean = mean(ztest))

vizit_predict <- data.frame("x" = as.vector(mesh_out_real$X), "y" = as.vector(mesh_out_real$Y), 
                            "z" = ztest)

predict_nn_plot <- ggplot(vizit_predict, aes(x, y, fill = z)) + 
  geom_tile() + 
  scale_fill_viridis_c(name = expression(C[L])) + 
  xlab("Angle of attack") + ylab("Camber") +
  ggtitle("Trained Neural Network") + 
  theme_bw() #+ 
  #theme(legend.position = "bottom"
```

Create and save the figure: 

```{r}
grid_plot + theme(legend.position = "none") + 
  predict_gpc_plot + theme(legend.position = "bottom") + 
  predict_nn_plot + theme(legend.position = "none") + 
  plot_annotation(tag_levels = "a")
dir.create("./results/plots/", showWarnings = F)
ggsave(plot = last_plot(), filename = "./results/plots/surrogate_compare_figure.pdf", 
       height = 3.0*1.5, width = 6.5*1.5)
```

The sections below will step you through the creation of each subplot of this figure through grid sampling, generalized polynomial chaos expansion, and training of a neural network from the original simulation data as produced by MATLAB in step 3. 

## Visualizing results of grid sampling

Actual results of the grid sampling across the three parameters can be plotted in a grid map to create a performance space. 

First, we start by defining the parameter ranges:

```{r}
Re_range <- c(1e6, 1e9)
aoa_range <- c(0, 15)
camber_range <- c(0.005, 0.20)
```

Next, we'll load the parameter range files produced in step 1 and then the results. Since the step 3 MATLAB code performs calculations in parallel, the simulations are out of order in the results file. We must reorder the results before binding them with the parameter data. 

```{r}
# Read in the results of the simulations:
grid_results <- read.csv("./results/grid_noLogRe_results.csv")
# Reorder the results so that they are in the same order as the parameter file:
grid_results <- grid_results[order(grid_results$sim), ]
# Read in the parameter set:
grid_params <- read.csv("./data/parameters/grid_Params_noLogRe.csv", header = F)
colnames(grid_params) <- c("Re", "aoa", "camber")
# Bind the parameters and results together:
grid_results <- cbind("sim" = grid_results$sim, grid_params, grid_results[, 2:4])
# Clean up 
rm(grid_params)
```

Here we can plot a 2D performance space of the coefficient of lift ($C_L$) at a constant Reynolds number ($Re = 500500000$). The resulting performance space is low resolution and simulations that failed (`NaN`) are represented as gray tiles. 

```{r}
grid_plot <- ggplot(grid_results[grid_results$Re == 500500000,], aes(aoa, camber, fill = CL)) +
  geom_tile() +
  scale_fill_viridis_c(name = expression(C[L])) + 
  xlab("Angle of attack") + ylab("Camber") +
  ggtitle("Grid sampling") + 
  theme_bw()
grid_plot
```


## Creating a surrogate with the generalized polynomial chaos model

This section will create and validate the gPC surrogate based on full simulations in step 3 (`3-xfoil_step_MALTAB.mlx`) based on the points selected in step 1 (`1-Generating-points.Rmd`).

Load custom functions: 

```{r}
source("./src/r-scripts/gPC_functions.R")
```

### Set the initial gPC parameters

In this section, we are generating the initial parameters for the gPC model. These are the same steps taken in step 1 (`1-Generating-points.Rmd`) and the gPC primer (`gPC_primer.Rmd`).

The initial parameters are:

 - `n`: the number of parameters in your data set. 
 - `p`: the order of polynomial you wish to use. Using lower values of `p` may not capture non-linear aspects of the performance space, but will require fewer full simulations. Higher `p` will require more simulations.

```{r set-gpc-parameters}
test_flag <- 1
n <- 3
p <- 6
cap_p <- nchoosek(n + p, p)
n_subset <- 343
```

Find collocation points, which are the roots of the Legendre polynomial chosen to by the number of parameters.

```{r collocation-points}
poly_roots <- Legendre_roots(p + 1, test_flag)
```

Here, we find all the combinations of all collocation points: 

```{r}
param_combo <- compute_all_collo_pt_combos(n, poly_roots, test_flag)
```


Here, we use multivariable Legendre polynomial ordering to generate the information matrices necessary for the gPC model.

```{r}
alpha_mat <- create_polynomial_ordering(n, p, test_flag)
info_mat <- create_info_matrix(n, p, cap_p, param_combo, alpha_mat, test_flag)
```

### Loading results from full simulations

There will be two sets of full simulations used, one for training and one for testing the gPC model. The non-log transformed Reynolds number set will be used for training, and the log-transformed Reynolds number set will be used for testing. The results output used to create the performance space will be coefficient of lift ($C_L$), but any one of the outputs can be swapped out to create a performance space.

Similar to the grid sampling, parameters in the set are run in parallel and recorded out of order. The results need to be reordered based on simulation number before being joined. We then remove simulations that failed to converge.

Furthermore, the polynomial ordering is slightly different between MATLAB and R. Since the MATLAB numbers were originally used, it's necessary to reorder the parameter set itself so that the results match up with the param_combo calculated by R in the section above. If you are producing the parameter sets and then creating surrogates entirely through R, this step is not needed.

```{r}
### Training data
# Loading parameters
train_gpc_params <- read.csv("./data/parameters/gPC_Params_noLogRe.csv",
                            header = F)
# Reorder vector according to previously run parameter set. Matlab outputs 
# parameter combo differently than R, and the original parameter set was generated
# with Matlab. This reordering step does not need to happen if parameter sets
# are generated only with R.
param_reorder <- reorder_param_data(train_gpc_params, Re_range, aoa_range, 
                                    camber_range, param_combo)

# Loading simulation results
train_gpc_results <- read.csv("./results/gpc_noLogRe_results.csv", header = T)
# Reorder results based on simulation number (to match csv file)
train_gpc_results <- train_gpc_results[order(train_gpc_results$sim), ]
# Create full data frame, output used will be coefficient of lift.
training_gpc_dat <- cbind("sim" = train_gpc_results$sim, train_gpc_params, 
                           "output" = train_gpc_results$CL)
# Reorder to match param_combo produced here
training_gpc_dat <- training_gpc_dat[order(param_reorder),]
# Reset simulation numbers
training_gpc_dat$sim <- seq(1,nrow(training_gpc_dat))
# Remove failed simulations
training_gpc_dat <- training_gpc_dat[!is.nan(training_gpc_dat$output), ]
colnames(training_gpc_dat) <- c( "sim","Re", "aoa", "camber", "output")
# Clean up
rm(train_gpc_params, train_gpc_results)
```

Now we load the testing data set. The matching between MATLAB and R here is unnecessary, so you'll notice that `reorder_param_data()` is not used here.

```{r}
### Testing data
# Loading parameters
test_gpc_params <- read.csv("./data/parameters/gPC_Params_LogRe.csv", 
                           header = F)
# Loading simulation results
test_gpc_results <- read.csv("./results/gpc_LogRe_results.csv", header = T)
# Reorder results based on simulation number
test_gpc_results <- test_gpc_results[order(test_gpc_results$sim), ]
# Create results data frame
testing_gpc_dat <- cbind("sim" = test_gpc_results$sim, test_gpc_params, 
                          "output" = test_gpc_results$CL)
colnames(testing_gpc_dat) <- c("sim", "Re", "aoa", "camber", "output")
# Remove failed simulations
testing_gpc_dat <- testing_gpc_dat[!is.nan(testing_gpc_dat$output),]
# Clean up
rm(test_gpc_results, test_gpc_params)
```


Since we removed failed simulations from the results, these must also be removed from the information matrix used to calculate gPC coefficients.

```{r}
info_mat <- info_mat[training_gpc_dat$sim,]
```

Calculate the pseudo-inverse of the info matrix:

```{r}
ps_inv <- inv(t(info_mat) %*% info_mat) %*% t(info_mat)
#temp_mat <- t(info_mat) %*% info_mat
#S <- svd(temp_mat)
#sigma <- diag(S$d, cap_p, cap_p)
```

Get gPC coefficients for the output (set as $C_L$ above):

```{r}
s_coeffs_CL <- ps_inv %*% training_gpc_dat$output
#s_coeffs_CL <- S$v %*% solve(sigma) %*% Conj(t.default(S$u)) %*% t(info_mat) %*% output_gpc
```

Save the coefficients and alpha matrix, these will be necessary for running the values in the gPC expansion (responsible for producing the surrogate performance space).

```{r}
dir_path <- paste0("./src/r-scripts/gpc-data-", Sys.Date())
dir.create(dir_path, showWarnings = F)
write.csv(s_coeffs_CL, paste0(dir_path,"/gPC_coefficients.csv"), 
          row.names = FALSE)
write.csv(alpha_mat, paste0(dir_path,"/gPC_alphamat.csv"), row.names = FALSE)
```

Now, we validate the gPC expansion by recovering the training data set. A data frame with the real and training data, as well as the relative error for each simulation, is also produced.

```{r}
gpc_CL <- gPC_expansion(training_gpc_dat, s_coeffs_CL, alpha_mat, Re_range, aoa_range, camber_range)

results_gpc_train <- data.frame("sim" = training_gpc_dat$sim, 
                             "gpc_train" = gpc_CL, 
                             "gpc_real" = training_gpc_dat$output,
                             "rel_error" = abs(gpc_CL - training_gpc_dat$output) /
                               abs(training_gpc_dat$output)
)


plot_gpc_train <- pivot_longer(results_gpc_train[,1:3], cols = !c("sim"))

```

Plot validation results. You'll notice that the training data is sitting on the real simulation data produced by XFOIL. This indicates successful reproduction of the training data set.

```{r}
p_1_train <- ggplot(plot_gpc_train, aes(x = sim, y = value, color = name)) + 
  geom_point() +
  ylab("Coefficent of Lift") + 
  theme_bw()
p_2_train <- ggplot(results_gpc_train, aes(rel_error*100)) +
  geom_histogram() + 
  ylab("PDF") + 
  xlab("Percent relative error") +
  theme_bw()
p_1_train / p_2_train
```

Error calculations also indicate very low percent error for the validation.

```{r}

print(paste("Mean relative error for Lift:",
            signif(mean(results_gpc_train$rel_error)*100, 
                   digits = 3), "%"))
print(paste("Max relative error for Lift:",
            signif(max(results_gpc_train$rel_error)*100, 
                   digits = 3), "%"))
```

Next, we'll test the gPC expansion with data from the test set, which is selected from a different distribution as the training set. A results data frame is constructed here as well. 

```{r}
gpc_CL_test <- gPC_expansion(testing_gpc_dat, s_coeffs_CL, alpha_mat, 
                             Re_range, aoa_range, camber_range)

results_gpc_test <- data.frame("sim" = testing_gpc_dat$sim, 
                             "gpc_test" = gpc_CL_test, 
                             "gpc_real" = testing_gpc_dat$output,
                             "rel_error" = abs(gpc_CL_test - testing_gpc_dat$output)/
                               abs(testing_gpc_dat$output)
)


plot_gpc_test <- pivot_longer(results_gpc_test[,1:3], cols = !c("sim"))

```

Again, the majority of test simulation values lay closely to the real values from XFOIL. This indicates successful testing of the gPC surrogate function.

```{r}
p_1_test <- ggplot(plot_gpc_test, aes(x = sim, y = value, color = name)) + 
  geom_point() +
  ylab("Coefficent of Lift") + 
  theme_bw()
p_2_test <- ggplot(results_gpc_test, aes(rel_error*100)) +
  geom_histogram() + 
  ylab("PDF") + 
  xlab("Percent relative error") +
  theme_bw()
p_1_test / p_2_test
```

Error calculations indicate low percent error (around 2%) between the surrogate functions and real values. 

```{r}

print(paste("Mean relative error for Lift:",
            signif(mean(results_gpc_test$rel_error)*100, 
                   digits = 3), "%"))
print(paste("Max relative error for Lift:",
            signif(max(results_gpc_test$rel_error)*100, 
                   digits = 3), "%"))
```

After successfully training and testing the gPC surrogate, we can use them to generate the performance space. This is done by creating an even grid of parameter values along two parameters (angle of attack and camber), while holding Reynolds number constant at 50050000 to match the other methods' plots. 

```{r}
# Load previously generated gPC coefficients and alpha mat
s_coeffs_CL <- as.matrix(read.csv("./src/r-scripts/gpc-data-2025-09-11/gPC_coefficients.csv"))
alpha_mat <- as.matrix(read.csv("./src/r-scripts/gpc-data-2025-09-11/gPC_alphamat.csv"))

# Generate surrogate performance space
y1_vec <- seq(aoa_range[1], aoa_range[2], length.out = 100)
z1_vec <- seq(camber_range[1], camber_range[2], length.out = 100)
mesh_out <- meshgrid(y1_vec, z1_vec)
y1_mesh <- mesh_out$X
z1_mesh <- mesh_out$Y
x1_const <- 500500000
x1_mesh <- matrix(x1_const, nrow = 100, ncol = 100)
surrogate_dat <- data.frame("Re" = as.vector(x1_mesh), "aoa" = as.vector(y1_mesh), 
                            "camber" = as.vector(z1_mesh))

surrogate_dat$CL_surrogate <- gPC_expansion(surrogate_dat, s_coeffs_CL, alpha_mat, 
                             Re_range, aoa_range, camber_range)

```

Plot the resulting surrogate performance space. 

```{r}
predict_gpc_plot <- ggplot(surrogate_dat, aes(aoa, camber, fill = CL_surrogate)) + 
  geom_tile() + 
  scale_fill_viridis_c(name = expression(C[L])) + 
  xlab("Angle of attack") + ylab("Camber") +
  ggtitle("gPC Expansion") + 
  theme_bw() #+ 
  #theme(legend.position = "bottom")
predict_gpc_plot
```

## Creating a surrogate with the Neural Network model

The parameter sets created in `1-Generating_points.RMD` that were then run through the full simulation of XFOIL will serve as the training data set for the neural network. The first step will be training the model. We'll also train the model with both the Re no-log transformed set and the Re log-transformed set.

Load custom functions: 

```{r}
source("./src/r-scripts/nn_functions.R")
```

Define inputs and outputs: 

```{r}
num_inputs <- 3
num_outputs <- 1
eps_error <- 0
testing <- F
```

### Prepare the data for training and testing

We'll load the non-log transformed parameter set and results used to run XFOIL. We'll reorder the results according to the order of the original parameter set so that the two data sets match up correctly. 

```{r}
train_params <- read.csv("./data/parameters/NN_Params_noLogRe.csv", header = F)
train_results <- read.csv("./results/NN_noLogRe_results.csv", header = T)
train_results <- train_results[order(train_results$sim),]
```


Next, combine the parameter set and CL results into a single matrix. Since there are several NaN produced by XFOIL, we need to remove those failed simulations. 

```{r}
training_data <- cbind(train_params, "output" = train_results$CL)
training_data <- training_data[!is.nan(training_data[,4]),]
```

We can define the number of training data based on the successful simulations. We'll also generate the testing data set based on that number of simulations:

```{r}
n_train <- nrow(training_data)
```

The testing data will be the log-transformed data set. 

```{r}
test_params <- read.csv("./data/parameters/gPC_Params_noLogRe.csv", header = F)
colnames(test_params) <- c("Re", "aoa", "camber")
test_results <- read.csv("./results/gPC_noLogRe_results.csv", header = T)
test_results <- test_results[order(test_results$sim),]
testing_data <- cbind(test_params, "output" = test_results$CL)
testing_data <- as.matrix(testing_data[!is.nan(testing_data[,4]),])
n_test <- nrow(testing_data)
```

The testing and training data now need to be scaled and standardized. This is an important step that helps train the models, which work best with data that are normally distributed. 

First, the parameter ranges are scaled such that their ranges are [0,1].

```{r}
scaled_params_train <- scale_params_data(training_data[,1:3], F)
scaled_params_test <- scale_params_data(testing_data[,1:3], T)
```

Next, the output data need to viewed to see what needs to happen. Using CL, we can see that the data are skewed and not normally distributed, so we will normalize and standardize the output for training. 

```{r warning=FALSE, message=FALSE}
ggplot(train_results, aes(CL)) + geom_histogram() + theme_bw()
```

There are a variety of options for transforming output data. Two are implemented in this code: the Box Cox and Box Cox Y transformations. For `CL`, we use the Box Cox Y option. For more information, these resources are helpful. 
Box Cox: https://reinec.medium.com/my-notes-handling-skewed-data-5984de303725 
Box Cox Y: https://www.jmp.com/support/help/en/18.2/index.shtml#page/jmp/boxcox-y-transformation.shtml


```{r}

# Box Cox Y transformation
lambda <- 1.7
train_output <- transform_output_data(training_data[,4], option = "box-cox-y", lambda)
test_output <- transform_output_data(testing_data[,4], option = "box-cox-y", lambda)

scale_output2 <- scale_output_data(train_output$trans_output, test_output$trans_output)
training_data_scaled <- cbind(scaled_params_train, scale_output2$train_output)
colnames(training_data_scaled) <- c("Re", "aoa", "camber", "CL")

testing_data_scaled <- cbind(scaled_params_test, scale_output2$test_output)
colnames(testing_data_scaled) <- c("Re", "aoa", "camber", "CL")
```

Replotting the data demonstrates that the data are now distributed more normally. 

```{r warning=FALSE, message=FALSE}
ggplot(training_data_scaled, aes(CL)) + geom_histogram() + theme_bw()
```

### Training the Neural Network

Next, we define hyperparameters. Each is noted with a comment below: 

```{r}
hyper_param_list <- list(
  "num_hidden_layer_neurons" = 200,  # number of neurons per layer (size of hidden layer)
  "learning_rate" = 0.55,            # learning rate ("lambda", Gradient Descent Step-size)
  "momentum" = 0.0,                  # momentum ("inertia factor" from previous Gradient Descent Step)
  "max_epochs" = 1500,               # max number of EPOCHS (forward prop + back prop in training through ALL Training Data)
  "batch_size" = 1,                  # number of samples per mini-batch in Stochastic Gradient Descent
  "print_interval" = 100,            # how often to print COST/ERROR info to screen during training
  "adaptive_step_size" = 0,          # 0 for no, 1 for yes (Barzilai_Borwein step)
  "num_inputs" = num_inputs,         # number of input parameters
  "regularization_flag" = 0,         # 0 for none, 1 for L1, 2 for L2
  "lam_regularize" = 8e-8            # Regularization coefficient, lambda
)

```

Train the artificial neural network. Save the trained model values as separate csv files in `./src/r-scripts/nn-data-date/` where date is the system date at the time the code was run. The step can take a good bit of time. 


```{r}
trained_model <- train_artificial_neural_network(training_data_scaled, testing_data_scaled,
                                                 hyper_param_list, testing)
save_trained_model_values(trained_model, scale_output2$min_z, scale_output2$max_z)
```

### Intepreting the success of training

Plot the cost/loss function versus iteration number: 

```{r}
plot_cost <- data.frame("iter" = 1:length(trained_model$cost_vec), 
                        "cost_vec" = trained_model$cost_vec)
ggplot(plot_cost, aes(iter, log10(cost_vec))) + 
  geom_point(col = "blue") + 
  theme_bw()
```

Comparing the training data set (generated by XFOIL simulations) to the same values predicted by the trained model validates the training process. These values are: 

```{r}
w1 <- trained_model$w1_save
w2 <- trained_model$w2_save
wend <- trained_model$wend_save
b1 <- trained_model$b1_save
b2 <- trained_model$b2_save
bend <- trained_model$bend_save
fp_results_valid <- forward_propagation(t(training_data_scaled[,1:num_inputs]), 
                                        w1, w2, wend, b1, b2, bend)
zvalid_scaled <- t(fp_results_valid$z_hat)
zvalid <- unscale_output_data(zvalid_scaled, scale_output2$min_z, scale_output2$max_z)
zvalid <- untransform_output_data(zvalid, option = "box-cox-y", lambda, 
                                  dat_mean = mean(training_data[,4]))

print_error_information(zvalid, training_data, "Validation", num_inputs)
```
Average ABS error is very low, indicating a successful training session. 

Next, another set of testing data (produced by XFOIL simulations) drawn from a different set of parameter combinations can be compared to the predicted values from the trained model to test the validity of the trained model. These values are:

```{r}
fp_results_test <- forward_propagation(t(testing_data_scaled[,1:num_inputs]), 
                                        w1, w2, wend, b1, b2, bend)
ztest_scaled <- t(fp_results_test$z_hat)
ztest <- unscale_output_data(ztest_scaled, scale_output2$min_z, scale_output2$max_z)
ztest <- untransform_output_data(ztest, option = "box-cox-y", lambda, 
                                  dat_mean = mean(testing_data[,4]))
print_error_information(ztest, testing_data, "Testing", num_inputs)
```
The relative error in this case is around 3% (varies with every model-training session), which is an acceptable amount of error between the model and real values in this system.

Both these steps can be visualized by plotting the data points (XFOIL simulation values) to the predicted values from the trained model for both training and testing:

```{r warning=FALSE, message=FALSE}
training_data_df <- data.frame("number" = rep(1:nrow(training_data),2), 
                              "function_value" = c(training_data[,(num_inputs+1)], zvalid), 
                              "type" = rep(c("data", "prediction"), each = nrow(training_data)))

testing_data_df <- data.frame("number" = rep(1:nrow(testing_data),2), 
                              "function_value" = c(testing_data[,(num_inputs+1)], ztest), 
                              "type" = rep(c("data", "prediction"), each = nrow(testing_data)))

train_plot <- ggplot(training_data_df, aes(x = number, y = function_value, color = type,
                                           size = type)) + 
  geom_point() + 
  scale_size_manual(values = c(2,1)) +
  xlab("Data point") + ylab("Function value") +
  ggtitle("Model Validation") +
  theme_bw()

test_plot <- ggplot(testing_data_df, aes(x = number, y = function_value, color = type,
                                         size = type)) + 
  geom_point() + 
  scale_size_manual(values = c(2,1)) +
  xlab("Data point") + ylab("Function value") +
  ggtitle("Model Testing") +
  theme_bw()

train_plot + test_plot + plot_layout(guides = "collect")
```

### Creating a surrogate with the trained-model weights

After training the model, we have a set of trained-model parameters that can be used to predict the value of any parameter combination within the ranges of the trained model. To create a 2D test slice to visualize trained neural network, we pass these through a function `create_2D` which uses the `forward_propagation` function to predict unknown values. This is extremely low cost compared to running individual simulations in XFOIL and can generate detailed performance spaces. 

```{r}
trained_model <- load_trained_model("./src/r-scripts/nn-data-2025-08-26/")
x1_const <- 500500000
x1_const_scaled <- 0.5
x1_mesh_scaled <- matrix(x1_const_scaled, nrow = 100, ncol = 100)
y1_vec <- seq(0, 15, length.out = 100)
z1_vec <- seq(0, 0.2, length.out = 100)
mesh_out <- meshgrid(line_fxn(y1_vec), line_fxn(z1_vec))
mesh_out_real <- meshgrid(y1_vec, z1_vec)
y1_mesh <- mesh_out$X
z1_mesh <- mesh_out$Y


viz_it <- create_2D(x1_mesh_scaled, y1_mesh, z1_mesh, trained_model, true_dat = F)

ztest <- unscale_output_data(as.vector(viz_it$f_prediction), 
                             scale_output2$min_z, scale_output2$max_z)
ztest <- untransform_output_data(ztest, option = "box-cox-y", lambda, 
                                  dat_mean = mean(testing_data[,4]))

vizit_predict <- data.frame("x" = as.vector(mesh_out_real$X), "y" = as.vector(mesh_out_real$Y), 
                            "z" = ztest)

```

The resulting performance space, over a 100 by 100 grid of angles of attack and airfoil cambers at $Re = 500$k, is plotted below. 

```{r}
predict_nn_plot <- ggplot(vizit_predict, aes(x, y, fill = z)) + 
  geom_tile() + 
  scale_fill_viridis_c(name = expression(C[L])) + 
  xlab("Angle of attack") + ylab("Camber") +
  ggtitle("Trained neural network") + 
  theme_bw() #+ 
  #theme(legend.position = "bottom")

predict_nn_plot +  plot_layout(guides = "collect") & theme(legend.position = "bottom")

```



