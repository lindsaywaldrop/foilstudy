---
title: "Creating Surrogates"
output:
  pdf_document: default
  html_notebook: default
---

```{r setup, echo=FALSE}
library(pracma)
library(spacefillr)
library(ggplot2)
library(patchwork)
library(tidyr)
```

This document walks through the process of creating performance spaces from each of the three methods (grid sampling, generalized polynomial chaos, and neural network) using the same number of simulations for each method (343 total) across three parameters (Reynolds number, angle of attack, and airfoil camber). The performance metric chosen is the coefficient of lift ($C_L$). The end result is a figure presented in the review paper comparing the resolutions of each method.

## Visualizing results of grid sampling

Actual results of the grid sampling across the three parameters can be plotted in a grid map to create a performance space. 

```{r}
Re_range <- c(1e6, 1e9)
aoa_range <- c(0, 15)
camber_range <- c(0.005, 0.20)
```

```{r}
# Read in the results of the simulations:
grid_results <- read.csv("./results/grid_noLogRe_results.csv")
# Reorder the results so that they are in the same order as the parameter file:
grid_results <- grid_results[order(grid_results$sim), ]
# Read in the parameter set:
grid_params <- read.csv("./data/parameters/grid_Params_noLogRe.csv", header = F)
colnames(grid_params) <- c("Re", "aoa", "camber")
# Bind the parameters and results together:
grid_results <- cbind("sim" = grid_results$sim, grid_params, grid_results[, 2:4])
# Clean up 
rm(grid_params)
```

Here we can plot a 2D performance space of the coefficient of lift ($C_L$) at a constant Reynolds number ($Re = 500500000$). The resulting performance space is low resolution and simulations that failed (`NaN`) are represented as gray tiles. 

```{r}
grid_plot <- ggplot(grid_results[grid_results$Re == 500500000,], aes(aoa, camber, fill = CL)) +
  geom_tile() +
  scale_fill_viridis_c(name = expression(C[L])) + 
  xlab("Angle of attack") + ylab("Camber") +
  ggtitle("Grid sampling") + 
  theme_bw()
grid_plot
```


## Creating a surrogate with the generalized polynomial chaos model

This section will create and validate the gPC surrogate based on full simulations in step 3 (`3-xfoil_step_MALTAB.mlx`) based on the points selected in step 1 (`1-Generating-points.Rmd`).

Load custom functions: 

```{r}
source("./src/r-scripts/gPC_functions.R")
```

### Set the initial gPC parameters

In this section, we are generating the initial parameters for the gPC model. These are the same steps taken in step 1 (`1-Generating-points.Rmd`) and the gPC primer (`gPC_primer.Rmd`).

The initial parameters are:

 - `n`: the number of parameters in your data set. 
 - `p`: the order of polynomial you wish to use. Using lower values of `p` may not capture non-linear aspects of the performance space, but will require fewer full simulations. Higher `p` will require more simulations.

```{r set-gpc-parameters}
test_flag <- 1
n <- 3
p <- 6
cap_p <- nchoosek(n + p, p)
n_subset <- 343
```

Find collocation points:

```{r collocation-points}
poly_roots <- Legendre_roots(p + 1, test_flag)
```

Find combinations of all collocation points: 

```{r}
param_combo <- compute_all_collo_pt_combos(n, poly_roots, test_flag)
```


Here, we use multivariable Legendre polynomial ordering to generate the information matrix necessary for the gPC model.

```{r}
alpha_mat <- create_polynomial_ordering(n, p, test_flag)
info_mat <- create_info_matrix(n, p, cap_p, param_combo, alpha_mat, test_flag)
```

### Loading results from full simulations

There will be two sets of full simulations used, one for training and one for testing the gPC model. 

```{r}
### Training data
# Loading parameters
train_gpc_params <- read.csv("./data/parameters/gPC_Params_noLogRe.csv",
                            header = F)
# Reorder vector according to previously run parameter set
param_reorder <- reorder_param_data(train_gpc_params, Re_range, aoa_range, 
                                    camber_range, param_combo)

# Loading simulation results
train_gpc_results <- read.csv("./results/gpc_noLogRe_results.csv", header = T)
# Reorder results based on simulation number (to match csv file)
train_gpc_results <- train_gpc_results[order(train_gpc_results$sim), ]
# Create full data frame
training_gpc_data <- cbind("sim" = train_gpc_results$sim, train_gpc_params, 
                           "output" = train_gpc_results$CL)
# Reorder to match param_combo produced here
training_gpc_data <- training_gpc_data[order(param_reorder),]
# Reset simulation numbers
training_gpc_data$sim <- seq(1,nrow(training_gpc_data))
# Remove failed simulations
training_gpc_data <- training_gpc_data[!is.nan(training_gpc_data$output), ]
colnames(training_gpc_data) <- c( "sim","Re", "aoa", "camber", "output")
# Clean up
rm(train_gpc_params, train_gpc_results)
```


```{r}
### Testing data
# Loading parameters
test_gpc_params <- read.csv("./data/parameters/gPC_Params_LogRe.csv", 
                           header = F)
# Loading simulation results
test_gpc_results <- read.csv("./results/gpc_LogRe_results.csv", header = T)
# Reorder results based on simulation number
test_gpc_results <- test_gpc_results[order(test_gpc_results$sim), ]
# Create results data frame
testing_gpc_data <- cbind("sim" = test_gpc_results$sim, test_gpc_params, 
                          "output" = test_gpc_results$CL)
# Remove failed simulations
testing_gpc_data <- testing_gpc_data[!is.nan(testing_gpc_data$output),]
# Clean up
rm(test_gpc_results, test_gpc_params)
```


Restrict matrices to only successful simulations: 

```{r}
# R data
info_mat <- info_mat[training_gpc_data$sim,]
train_gpc_params <- training_gpc_data[,2:4]
```

Extract output data, here it is coefficient of lift (`CL`):

```{r }
output_gpc <- training_gpc_data$output
```

Calculate the pseudo-inverse of the info matrix:

```{r}
ps_inv <- inv(t(info_mat) %*% info_mat) %*% t(info_mat)
#temp_mat <- t(info_mat) %*% info_mat
#S <- svd(temp_mat)
#sigma <- diag(S$d, cap_p, cap_p)
```

Get gPC coefficients for the output: 

```{r}
s_coeffs_CL <- ps_inv %*% output_gpc
#s_coeffs_CL <- S$v %*% solve(sigma) %*% Conj(t.default(S$u)) %*% t(info_mat) %*% output_gpc
```

Now, we validate the gPC expansion by recovering the training data set. 

```{r}
gpc_CL <- rep(NA, nrow(train_gpc_params))

for(j in 1:nrow(train_gpc_params)){
  Re_test <- train_gpc_params[j,1]
  aoa_test <- train_gpc_params[j,2]
  camber_test <- train_gpc_params[j,3]
  
  a <- 1e6
  b <- 1e9
  val_orig <- Re_test
  re_val <- transform_values_to_minus1_plus1(val_orig, a, b)
  
  a <- 0.0
  b <- 15.0
  val_orig <- aoa_test
  aoa_val <- transform_values_to_minus1_plus1(val_orig, a, b)
  
  a <- 0.005
  b <- 0.20
  val_orig <- camber_test
  camber_val <- transform_values_to_minus1_plus1(val_orig, a, b)
  
  vec <- c(re_val, aoa_val, camber_val)
  
  # Evaluate multidim Legendre Polys for output
  gpc_CL[j] <- eval_multidim_Legendre_poly(s_coeffs_CL, alpha_mat, vec)
  
}

plot_gpc_train <- data.frame("sim" = training_gpc_data$sim, 
                             "gpc_train" = gpc_CL, 
                             "gpc_real" = output_gpc)
plot_gpc_train <- pivot_longer(plot_gpc_train, cols = !"sim")
```

Plot it: 

```{r}
ggplot(plot_gpc_train, aes(x = sim, y = value, color = name)) + 
  geom_point() +
  ylab("Coefficent of Lift") + 
  theme_bw()
```

## Creating a surrogate with the Neural Network model

The parameter sets created in `1-Generating_points.RMD` that were then run through the full simulation of XFOIL will serve as the training data set for the neural network. The first step will be training the model. We'll also train the model with both the Re no-log transformed set and the Re log-transformed set.

Load custom functions: 

```{r}
source("./src/r-scripts/nn_functions.R")
```

Define inputs and outputs: 

```{r}
num_inputs <- 3
num_outputs <- 1
eps_error <- 0
testing <- F
```

### Prepare the data for training and testing

We'll load the non-log transformed parameter set and results used to run XFOIL. We'll reorder the results according to the order of the original parameter set so that the two data sets match up correctly. 

```{r}
train_params <- read.csv("./data/parameters/NN_Params_noLogRe.csv", header = F)
train_results <- read.csv("./results/NN_noLogRe_results.csv", header = T)
train_results <- train_results[order(train_results$sim),]
```


Next, combine the parameter set and CL results into a single matrix. Since there are several NaN produced by XFOIL, we need to remove those failed simulations. 

```{r}
training_data <- cbind(train_params, "output" = train_results$CL)
training_data <- training_data[!is.nan(training_data[,4]),]
```

We can define the number of training data based on the successful simulations. We'll also generate the testing data set based on that number of simulations:

```{r}
n_train <- nrow(training_data)
```

The testing data will be the log-transformed data set. 

```{r}
test_params <- read.csv("./data/parameters/gPC_Params_noLogRe.csv", header = F)
colnames(test_params) <- c("Re", "aoa", "camber")
test_results <- read.csv("./results/gPC_noLogRe_results.csv", header = T)
test_results <- test_results[order(test_results$sim),]
testing_data <- cbind(test_params, "output" = test_results$CL)
testing_data <- as.matrix(testing_data[!is.nan(testing_data[,4]),])
n_test <- nrow(testing_data)
```

The testing and training data now need to be scaled and standardized. This is an important step that helps train the models, which work best with data that are normally distributed. 

First, the parameter ranges are scaled such that their ranges are [0,1].

```{r}
scaled_params_train <- scale_params_data(training_data[,1:3], F)
scaled_params_test <- scale_params_data(testing_data[,1:3], T)
```

Next, the output data need to viewed to see what needs to happen. Using CL, we can see that the data are skewed and not normally distributed, so we will normalize and standardize the output for training. 

```{r warning=FALSE, message=FALSE}
ggplot(train_results, aes(CL)) + geom_histogram() + theme_bw()
```

There are a variety of options for transforming output data. Two are implemented in this code: the Box Cox and Box Cox Y transformations. For `CL`, we use the Box Cox Y option. For more information, these resources are helpful. 
Box Cox: https://reinec.medium.com/my-notes-handling-skewed-data-5984de303725 
Box Cox Y: https://www.jmp.com/support/help/en/18.2/index.shtml#page/jmp/boxcox-y-transformation.shtml


```{r}

# Box Cox Y transformation
lambda <- 1.7
train_output <- transform_output_data(training_data[,4], option = "box-cox-y", lambda)
test_output <- transform_output_data(testing_data[,4], option = "box-cox-y", lambda)

scale_output2 <- scale_output_data(train_output$trans_output, test_output$trans_output)
training_data_scaled <- cbind(scaled_params_train, scale_output2$train_output)
colnames(training_data_scaled) <- c("Re", "aoa", "camber", "CL")

testing_data_scaled <- cbind(scaled_params_test, scale_output2$test_output)
colnames(testing_data_scaled) <- c("Re", "aoa", "camber", "CL")
```

Replotting the data demonstrates that the data are now distributed more normally. 

```{r warning=FALSE, message=FALSE}
ggplot(training_data_scaled, aes(CL)) + geom_histogram() + theme_bw()
```

### Training the Neural Network

Next, we define hyperparameters. Each is noted with a comment below: 

```{r}
hyper_param_list <- list(
  "num_hidden_layer_neurons" = 200,  # number of neurons per layer (size of hidden layer)
  "learning_rate" = 0.55,            # learning rate ("lambda", Gradient Descent Step-size)
  "momentum" = 0.0,                  # momentum ("inertia factor" from previous Gradient Descent Step)
  "max_epochs" = 1500,               # max number of EPOCHS (forward prop + back prop in training through ALL Training Data)
  "batch_size" = 1,                  # number of samples per mini-batch in Stochastic Gradient Descent
  "print_interval" = 100,            # how often to print COST/ERROR info to screen during training
  "adaptive_step_size" = 0,          # 0 for no, 1 for yes (Barzilai_Borwein step)
  "num_inputs" = num_inputs,         # number of input parameters
  "regularization_flag" = 0,         # 0 for none, 1 for L1, 2 for L2
  "lam_regularize" = 8e-8            # Regularization coefficient, lambda
)

```

Train the artificial neural network. Save the trained model values as separate csv files in `./src/r-scripts/nn-data-date/` where date is the system date at the time the code was run. The step can take a good bit of time. 


```{r}
trained_model <- train_artificial_neural_network(training_data_scaled, testing_data_scaled,
                                                 hyper_param_list, testing)
save_trained_model_values(trained_model)
```

### Intepreting the success of training

Plot the cost/loss function versus iteration number: 

```{r}
plot_cost <- data.frame("iter" = 1:length(trained_model$cost_vec), 
                        "cost_vec" = trained_model$cost_vec)
ggplot(plot_cost, aes(iter, log10(cost_vec))) + 
  geom_point(col = "blue") + 
  theme_bw()
```

Comparing the training data set (generated by XFOIL simulations) to the same values predicted by the trained model validates the training process. These values are: 

```{r}
w1 <- trained_model$w1_save
w2 <- trained_model$w2_save
wend <- trained_model$wend_save
b1 <- trained_model$b1_save
b2 <- trained_model$b2_save
bend <- trained_model$bend_save
fp_results_valid <- forward_propagation(t(training_data_scaled[,1:num_inputs]), 
                                        w1, w2, wend, b1, b2, bend)
zvalid_scaled <- t(fp_results_valid$z_hat)
zvalid <- unscale_output_data(zvalid_scaled, scale_output2$min_z, scale_output2$max_z)
zvalid <- untransform_output_data(zvalid, option = "box-cox-y", lambda, 
                                  dat_mean = mean(training_data[,4]))

print_error_information(zvalid, training_data, "Validation", num_inputs)
```
Average ABS error is very low, indicating a successful training session. 

Next, another set of testing data (produced by XFOIL simulations) drawn from a different set of parameter combinations can be compared to the predicted values from the trained model to test the validity of the trained model. These values are:

```{r}
fp_results_test <- forward_propagation(t(testing_data_scaled[,1:num_inputs]), 
                                        w1, w2, wend, b1, b2, bend)
ztest_scaled <- t(fp_results_test$z_hat)
ztest <- unscale_output_data(ztest_scaled, scale_output2$min_z, scale_output2$max_z)
ztest <- untransform_output_data(ztest, option = "box-cox-y", lambda, 
                                  dat_mean = mean(testing_data[,4]))
print_error_information(ztest, testing_data, "Testing", num_inputs)
```
The relative error in this case is around 3% (varies with every model-training session), which is an acceptable amount of error between the model and real values in this system.

Both these steps can be visualized by plotting the data points (XFOIL simulation values) to the predicted values from the trained model for both training and testing:

```{r warning=FALSE, message=FALSE}
training_data_df <- data.frame("number" = rep(1:nrow(training_data),2), 
                              "function_value" = c(training_data[,(num_inputs+1)], zvalid), 
                              "type" = rep(c("data", "prediction"), each = nrow(training_data)))

testing_data_df <- data.frame("number" = rep(1:nrow(testing_data),2), 
                              "function_value" = c(testing_data[,(num_inputs+1)], ztest), 
                              "type" = rep(c("data", "prediction"), each = nrow(testing_data)))

train_plot <- ggplot(training_data_df, aes(x = number, y = function_value, color = type,
                                           size = type)) + 
  geom_point() + 
  scale_size_manual(values = c(2,1)) +
  xlab("Data point") + ylab("Function value") +
  ggtitle("Model Validation") +
  theme_bw()

test_plot <- ggplot(testing_data_df, aes(x = number, y = function_value, color = type,
                                         size = type)) + 
  geom_point() + 
  scale_size_manual(values = c(2,1)) +
  xlab("Data point") + ylab("Function value") +
  ggtitle("Model Testing") +
  theme_bw()

train_plot + test_plot + plot_layout(guides = "collect")
```

### Creating a surrogate with the trained-model weights

After training the model, we have a set of trained-model parameters that can be used to predict the value of any parameter combination within the ranges of the trained model. To create a 2D test slice to visualize trained neural network, we pass these through a function `create_2D` which uses the `forward_propagation` function to predict unknown values. This is extremely low cost compared to running individual simulations in XFOIL and can generate detailed performance spaces. 

```{r}
trained_model <- load_trained_model("./src/r-scripts/nn-data-2025-08-26/")
x1_const <- 500500000
x1_const_scaled <- 0.5
x1_mesh_scaled <- matrix(x1_const_scaled, nrow = 100, ncol = 100)
y1_vec <- seq(0, 15, length.out = 100)
z1_vec <- seq(0, 0.2, length.out = 100)
mesh_out <- meshgrid(line_fxn(y1_vec), line_fxn(z1_vec))
mesh_out_real <- meshgrid(y1_vec, z1_vec)
y1_mesh <- mesh_out$X
z1_mesh <- mesh_out$Y


viz_it <- create_2D(x1_mesh_scaled, y1_mesh, z1_mesh, trained_model, true_dat = F)

ztest <- unscale_output_data(as.vector(viz_it$f_prediction), 
                             scale_output2$min_z, scale_output2$max_z)
ztest <- untransform_output_data(ztest, option = "box-cox-y", lambda, 
                                  dat_mean = mean(testing_data[,4]))

vizit_predict <- data.frame("x" = as.vector(mesh_out_real$X), "y" = as.vector(mesh_out_real$Y), 
                            "z" = ztest)

```

The resulting performance space, over a 100 by 100 grid of angles of attack and airfoil cambers at $Re = 500$k, is plotted below. 

```{r}
predict_nn_plot <- ggplot(vizit_predict, aes(x, y, fill = z)) + 
  geom_tile() + 
  scale_fill_viridis_c(name = expression(C[L])) + 
  xlab("Angle of attack") + ylab("Camber") +
  ggtitle("Trained neural network") + 
  theme_bw() #+ 
  #theme(legend.position = "bottom")

predict_nn_plot +  plot_layout(guides = "collect") & theme(legend.position = "bottom")

```

## Figure from publication

The figure from the review paper consists of each of the above methods plotted together. 

```{r}
grid_plot + theme(legend.position = "none") + predict_nn_plot +
  plot_annotation(tag_levels = "a")
dir.create("./results/plots/", showWarnings = F)
ggsave(plot = last_plot(), filename = "./results/plots/surrogate_compare_figure.pdf", 
       height = 3.5*1.5, width = 6.5*1.5)
```

