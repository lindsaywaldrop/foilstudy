---
title: "Generating parameter spaces using R"
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup}
library(pracma)
library(spacefillr)
library(ggplot2)
library(patchwork)
```

## Parameters of interest and their ranges 

In this study, we examine the output of coefficients of lift and drag on airfoils as a response to changes in the Reynolds number, angle of attack, and airfoil camber. We use the following parameter ranges to establish our parameter sampling spaces: 

 * Reynolds number: 1e6 to 1e9
 * Angle of Attack: 0 to 15
 * Camber: 0.005 to 0.20 

```{r}
Re_range <- c(1e6, 1e9)
aoa_range <- c(0, 15)
camber_range <- c(0.005, 0.20)
```

We use three methods for generating parameter sampling spaces: __grid sampling__, __generalized polynomial chaos__, and __neural network sampling__. Each of these are generated in the following sections of this code document and can also be found in the MATLAB script `./doc/Generating_points.MATLAB.mlx`. Output from this code will be csv files located in `./data/parameters/`.

## Producing the grid sampled parameter space

This is equivalent code to the grid sampling in the MATLAB script `./doc/Generating_points.MATLAB.mlx`, here reproduced in R. 

Load custom functions: 

```{r}
source("./src/r-scripts/grid_functions.R")
```

Since all methods use the same number of simulations, the uniform spacing of our grid will need to be divided by the steps of the cubic root of 343: 

```{r}
steps <- nthroot(343, 3)
grid_spacing_Re <- (Re_range[2] - Re_range[1])/(steps - 1)
grid_spacing_aoa <- (aoa_range[2] - aoa_range[1])/(steps - 1)
grid_spacing_camber <- (camber_range[2] - camber_range[1])/(steps - 1)
```

Create a vector of the individual values that will be run for each parameter:

```{r}
re_vec <- seq(Re_range[1], Re_range[2], by = grid_spacing_Re)
aoa_vec <- seq(aoa_range[1], aoa_range[2], by = grid_spacing_aoa)
camber_vec <- seq(camber_range[1], camber_range[2], by = grid_spacing_camber)
```

Extend all parameters to create a full parameter combination set:

```{r}
grid_3d <- mesh_array(re_vec, aoa_vec, camber_vec)
```

Reshape each individual parameter array into a 1D vector, then combine them into a 343 x 3 data frame:

```{r}
params_grid <- data.frame("Re" = as.vector(grid_3d[[1]]),
                          "aoa" = as.vector(grid_3d[[2]]), 
                          "camber" = as.vector(grid_3d[[3]]))
```

Save the resulting data frame:

```{r}
write.table(signif(params_grid, digits = 5), file = paste0("./data/parameters/grid_Params_noLogRe.csv"), 
              row.names = F, col.names = F, sep = ",")
```

## Producing gPC parameter space

This work is a validated translation of Nick Battista's MATLAB code into R. This section steps through the entire process found in `./src/matlab/USER_RUN_gPC_Expansion.m` that is called as a function in the MATLAB live script `./doc/Generating_points_MATLAB.mlx` in R. 

Load custom functions: 

```{r}
source("./src/r-scripts/gPC_functions.R")
source("./src/r-scripts/gpc_user_specified_model.R")
```

Initial parameters: 

 - `test_flag` will test the results against expected values produced by the code in MATLAB.`TRUE` should be displayed in the output of each code chunk (at least once) if testing is successful. If you wish to intrinsically validate this code, use `1` to test, otherwise `0` will turn testing off. 

```{r initial-parameters}
test_flag <- 0
```

Set gPC parameters:

 - `n`: the number of parameters in your data set. 
 - `p`: the order of polynomial you wish to use. Using lower values of `p` may not capture non-linear aspects of the performance space, but will require fewer full simulations. Higher `p` will require more simulations.

```{r set-gpc-parameters}
n <- 3
p <- 6
cap_p <- nchoosek(n + p, p)
```

Find collocation points:

```{r collocation-points}
poly_roots <- Legendre_roots(p + 1, test_flag)
```

Find combinations of all collocation points: 

```{r}
param_combo <- compute_all_collo_pt_combos(n, poly_roots, test_flag)
```

Get subset of all parameter combinations for least squares: 

```{r}
n_subset <- 1 * (n - 1) * cap_p
param_combo_subset <- sample_parameter_combos(n_subset, param_combo, test_flag)
```

You can use either the full set `param_combo` or the smaller subset `param_combo_subset` for running your full simulations. Here, we use the full set. 

### Evaluate and test parameter combinations

Multivariable Legendre polynomial ordering:

```{r}
alpha_mat <- create_polynomial_ordering(n, p, test_flag)
```

Create information matrix: 

```{r}
info_mat <- create_info_matrix(n, p, cap_p, param_combo_subset, alpha_mat, test_flag)
```

Evaluate user-specified model for particular parameter combos:

```{r}
Y <- rep(0, nrow(param_combo_subset))
for(i in 1:nrow(param_combo_subset)){
  Y[i] <- user_specified_model(param_combo_subset[i, ])
}

# testing against matlab values:
if(test_flag == 1){
  matlab_Y <- read.csv("./test/gpc_matlab/Y.csv", header = F)
  all.equal(Y, matlab_Y[, 1])
}
```

Solve for coefficients via least-squares with SVD:

```{r}
temp_mat <- t(info_mat) %*% info_mat
S <- svd(temp_mat)
sigma <- diag(S$d, cap_p, cap_p)

# testing against matlab values:
if(test_flag == 1){
  matlab_sigma <- as.matrix(read.csv("./test/gpc_matlab/sigma.csv", header = F))
  all.equal(sigma, matlab_sigma, check.attributes = F)
}

s_coeffs <- S$v %*% solve(sigma) %*% Conj(t.default(S$u)) %*% t(info_mat) %*% Y

# testing against matlab values:
if(test_flag == 1){
  matlab_sCoeffs <- as.matrix(read.csv("./test/gpc_matlab/s_coeffs.csv", header = F))
  all.equal(s_coeffs, matlab_sCoeffs, check.attributes = F)
}
```

Compute errors between gPC surrogate and true model function:

```{r}
sobol_seed <- 14
test_errors <- compute_validation_and_testing_error(n, s_coeffs, alpha_mat, 
                                     param_combo_subset, sobol_seed)
```

Report results:

```{r echo = FALSE}
err_rel_train <- abs(test_errors$training$gpc_dat - test_errors$training$real_dat) / abs(test_errors$training$real_dat) * 100
print("~*~._.~*~ Training set ~*~._.~*~")
print(paste("median error:", round(median(err_rel_train, na.rm = T), digits = 2)))
print(paste(c("min error:", "max error:"), round(range(err_rel_train), digits = 2)))
print(paste("standard deviation:", round(sd(err_rel_train, na.rm = T),digits = 2)))

err_rel_test <- abs(test_errors$testing$gpc_dat - test_errors$testing$real_dat) / abs(test_errors$testing$real_dat) * 100
print("~*~._.~*~ Testing set ~*~._.~*~");
print(paste("median error:", round(median(err_rel_test, na.rm = T), digits = 2)))
print(paste(c("min error:", "max error:"), round(range(err_rel_test), digits = 2)))
print(paste("standard deviation:", round(sd(err_rel_test, na.rm = T), digits = 2)))
```

Testing expansion on 2D subspace:

```{r}
plots <- test_gpc_expansion(s_coeffs, alpha_mat)

plots[[1]] + plots[[2]] + plot_layout(guides = "collect")
```

Compute the Sobol indices: 

```{r}
sobols <- compute_sobol_indices(s_coeffs, alpha_mat)
```

Report Sobol Indices:

```{r echo=F}
print("~*~._.~*~ 1st order Sobol Indices ~*~._.~*~")
print(paste0("s", 1:3, ": ", round(sobols[["s_first"]], digits = 4)))
print("~*~._.~*~ 2nd order Sobol Indices ~*~._.~*~")
print(paste0("s", c(12, 13, 23), ": ", round(sobols[["s2nd"]][!is.na(sobols[["s2nd"]])], 
                                             digits = 4)))
print("~*~._.~*~ s123 Sobol index ~*~._.~*~")
print(round(sobols[["s123"]][1], digits = 4))
print("~*~._.~*~ Total-order Sobol indices ~*~._.~*~")
print(paste0("sT", 1:3, ": ", round(sobols[["s_total"]], digits = 4)))
```

### Modify gPC parameter combinations to fit chose parameter ranges

Once the gPC parameter combinations are generated, you can re-scale the range of each parameter range to fit your particular parameter range of interest. Below is a demonstration of how to accomplish this for the foil study with the ranges defined above: 

```{r}
final_set <- data.frame("Re" = rep(NA, nrow(param_combo)), 
                        "aoa" = rep(NA, nrow(param_combo)),
                        "camber" = rep(NA, nrow(param_combo)))
```
 
Transform `aoa` and `camber`: 

```{r}
final_set$aoa <- ((param_combo[,2] + 1)/2)*max(aoa_range)
final_set$camber <-  (((param_combo[,3] + 1)/2))*(max(camber_range)  - min(camber_range)) + min(camber_range)
```

Note that the Reynolds number has to options for gPC and NN: a log-transformed range and a non-log transformed range. This is to accommodate the very large range of numbers (three orders of magnitude). 

Transform the first column of the param_combo to fit the Re range for non-log and log transformed: 

```{r}
Re_nolog <- (((param_combo[,1] + 1)/2))*(max(Re_range)  - min(Re_range)) + min(Re_range)
Re_log <- (((param_combo[,1] + 1)/2))*(log10(max(Re_range))  - log10(min(Re_range))) + log10(min(Re_range))
Re_log <- 10^(Re_log)
```

Now set the Re preference and save the resulting data frames: 

```{r}
# Log transformed set
final_set$Re <- Re_log
write.table(signif(final_set, digits = 5), file = paste0("./data/parameters/gPC_Params_LogRe.csv"), 
              row.names = F, col.names = F, sep = ",")
# Non-log transformed set
final_set$Re <- Re_nolog
write.table(signif(final_set, digits = 5), file = paste0("./data/parameters/gPC_Params_noLogRe.csv"), 
              row.names = F, col.names = F, sep = ",")
```

## Producing the Neural Network parameter space

At this time, the neural network is only available in MATLAB. Please see the MATLAB live script `./doc/Generating_points_Matlab.mlx` for a functioning NN point generation. 

This is R code translated from Nick Battista's MATLAB code included in the code repository. 

Load custom functions: 

```{r}
source("./src/r-scripts/nn_functions.R")
```

The first step is to get training and testing data. This involves deciding how you're sampling for each testing and training data sets and the size of each. 

Define inputs and outputs: 

```{r}
num_inputs <- 3
num_outputs <- 1
```

Define the number of training and testing samples, and the epsilon percent error to add on average:
```{r}
n_train <- 500
n_test <- 500
eps_error <- 0
testing <- F
```

Generate training and testing data: 

```{r}
sobol_seed <- 14
train_test_output <- get_training_and_test_data(num_inputs, n_train, n_test, eps_error, sobol_seed, 
                                                testing)

training_data <- cbind(train_test_output$training_data, train_test_output$train_output)
testing_data <- cbind(train_test_output$test_data, train_test_output$test_output)

dir.create("./src/r-scripts/nn-data", showWarnings = F)
write.table(training_data, file = "./src/r-scripts/nn-data/training_data.csv", 
          row.names = F, col.names = F, sep = ",")
write.table(testing_data, file = "./src/r-scripts/nn-data/testing_data.csv", 
          row.names = F, col.names = F, sep = ",")

training_data_scaled <- training_data
testing_data_scaled <- testing_data
```

Next, we define hyperparameters. Each is noted with a comment below: 

```{r}
hyper_param_list <- list(
  "num_hidden_layer_neurons" = 200,  # number of neurons per layer (size of hidden layer)
  "learning_rate" = 0.55,            # learning rate ("lambda", Gradient Descent Step-size)
  "momentum" = 0.0,                  # momentum ("inertia factor" from previous Gradient Descent Step)
  "max_epochs" = 1500,               # max number of EPOCHS (forward prop + back prop in training through ALL Training Data)
  "batch_size" = 1,                  # number of samples per mini-batch in Stochastic Gradient Descent
  "print_interval" = 100,            # how often to print COST/ERROR info to screen during training
  "adaptive_step_size" = 0,          # 0 for no, 1 for yes (Barzilai_Borwein step)
  "num_inputs" = num_inputs,         # number of input parameters
  "regularization_flag" = 0,         # 0 for none, 1 for L1, 2 for L2
  "lam_regularize" = 8e-8            # Regularization coefficient, lambda
)

```


Train the artificial neural network: 

```{r}
trained_model <- train_artificial_neural_network(training_data_scaled, testing_data_scaled, hyper_param_list, testing)
```



Saving the trained model values as separate csv files in `./src/r-scripts/nn-data/`: 

```{r}
save_trained_model_values(trained_model)
```

Plot the cost/loss function versus iteration number: 

```{r}
plot_cost <- data.frame("iter" = 1:length(trained_model$cost_vec), 
                        "cost_vec" = trained_model$cost_vec)
ggplot(plot_cost, aes(iter, log10(cost_vec))) + 
  geom_point(col = "blue") + 
  theme_bw()
```
Model validation: perform forward propagation on training data set to get model predicted output values. (Trying to recover the data set.)

```{r}
w1 <- trained_model$w1_save
w2 <- trained_model$w2_save
wend <- trained_model$wend_save
b1 <- trained_model$b1_save
b2 <- trained_model$b2_save
bend <- trained_model$bend_save
fp_results_valid <- forward_propagation(t(training_data_scaled[,1:num_inputs]), w1, w2, wend,
                                  b1, b2, bend)
zvalid_scaled <- t(fp_results_valid$z_hat)
if(testing){
  zvalid_scaled_matlab <- as.matrix(read.csv("./test/nn_matlab/Matlab_zvalid_scaled.csv", header = F))
  all.equal(zvalid_scaled[,1], zvalid_scaled_matlab[,1])
}
zvalid <- zvalid_scaled
```

Model testing: perform forward propagation on the test data to get model predicted output values (to test model against independent test data).

```{r}
fp_results_test <- forward_propagation(t(testing_data_scaled[,1:num_inputs]), w1, w2, wend,
                                       b1, b2, bend)
ztest_scaled <- t(fp_results_test$z_hat)
if(testing){
  ztest_scaled_matlab <- as.matrix(read.csv("./test/nn_matlab/Matlab_ztest_scaled.csv", header = F))
  all.equal(ztest_scaled[,1], ztest_scaled_matlab[,1])
}
ztest <- ztest_scaled
```

Stopped here, everything looks good so far. 

```{r}
training_data[,ncol(training_data)] <- train_test_output$train_output
testing_data[,ncol(testing_data)] <- train_test_output$test_output
```

Print error analysis:

```{r}
print_error_information(zvalid, training_data, "Validation", num_inputs)
print_error_information(ztest, testing_data, "Testing", num_inputs)
```

Visualizing results of validation and testing: 

```{r}
testing_data_df <- data.frame("number" = rep(1:nrow(testing_data),2), 
                              "function_value" = c(testing_data[,(num_inputs+1)], ztest), 
                              "type" = rep(c("data", "prediction"), each = nrow(testing_data)))

training_data_df <- data.frame("number" = rep(1:nrow(training_data),2), 
                              "function_value" = c(training_data[,(num_inputs+1)], zvalid), 
                              "type" = rep(c("data", "prediction"), each = nrow(training_data)))

test_plot <- ggplot(testing_data_df, aes(x = number, y = function_value, color = type,
                                         size = type)) + 
  geom_point() + 
  scale_size_manual(values = c(2,1)) +
  xlab("Data point") + ylab("Function value") +
  ggtitle("Model Testing") +
  theme_bw()

train_plot <- ggplot(training_data_df, aes(x = number, y = function_value, color = type,
                                           size = type)) + 
  geom_point() + 
  scale_size_manual(values = c(2,1)) +
  xlab("Data point") + ylab("Function value") +
  ggtitle("Model Validation") +
  theme_bw()

train_plot + test_plot + plot_layout(guides = "collect")
```

Create a 2D test slice to visualize trained NN: 

```{r}
x1_vec <- seq(0, 1, length.out = 100)
y1_vec <- seq(0, 1, length.out = 100)
mesh_out <- meshgrid(x1_vec, y1_vec)
x1_mesh <- mesh_out$X
y1_mesh <- mesh_out$Y
z_const <- 0.5


viz_it <- create_2D(x1_mesh, y1_mesh, z_const, trained_model, train_test_output, eps_error)

vizit_predict <- data.frame("x" = as.vector(viz_it$x), "y" = as.vector(viz_it$y), 
                            "z" = as.vector(viz_it$f_prediction))
vizit_real <- data.frame("x" = as.vector(viz_it$x), "y" = as.vector(viz_it$y), 
                            "z" = as.vector(viz_it$f_real))

vizit_relerror <- data.frame("x" = as.vector(viz_it$x), "y" = as.vector(viz_it$y), 
                            "z" = as.vector(viz_it$f_prediction) - as.vector(viz_it$f_real))
```

Plot the results: 

```{r}
predict_plot <- ggplot(vizit_predict, aes(x, y, fill = z)) + 
  geom_tile() + 
  scale_fill_viridis_c() + 
  ggtitle("Function Prediction by trained\nneural network") + 
  theme_bw() + 
  theme(legend.position = "bottom")

real_plot <- ggplot(vizit_real, aes(x, y, fill = z)) + 
  geom_tile() + 
  scale_fill_viridis_c() + 
  ggtitle("True Function values") + 
  theme_bw() + 
  theme(legend.position = "bottom")

relerror_plot <- ggplot(vizit_relerror, aes(x, y, fill = z)) + 
  geom_tile() + 
  scale_fill_viridis_c() + 
  ggtitle("Relative error values") + 
  theme_bw() 

predict_plot + real_plot + plot_layout(guides = "collect") & theme(legend.position = "bottom")
relerror_plot 
```


## Next Steps... 

The next step is to run the `Creating_cambers.Rmd` in RStudio to generate the airfoil files. 

